{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train iid model on a subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load data of 2006, 2008 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading set: ../datasets/Kyoto-2016_AnoShift/subset/2006_subset.parquet\n",
      "Loading set: ../datasets/Kyoto-2016_AnoShift/subset/2008_subset.parquet\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from data_processor.data_loader import split_set\n",
    "\n",
    "\n",
    "train_set1 = \"2006\"\n",
    "test_set = \"2008\"\n",
    "\n",
    "ds_size = \"subset\"\n",
    "label_col_name = '18'\n",
    "label_col_pos_val = '1'\n",
    "\n",
    "# We only keep features 0 to 13\n",
    "cols = [str(i) for i in range(14)]\n",
    "\n",
    "def prepare_set(year, ds_size, is_test_set=False):\n",
    "    keep_cols = cols\n",
    "\n",
    "    if is_test_set:\n",
    "        keep_cols += [label_col_name, ]\n",
    "\n",
    "    df_year_path = f\"../datasets/Kyoto-2016_AnoShift/{ds_size}/{year}_{ds_size}.parquet\"\n",
    "    print(\"Loading set:\", df_year_path)\n",
    "\n",
    "    df_year = pd.read_parquet(df_year_path)\n",
    "    df_year = df_year.drop(columns=list(set(df_year.columns) - set(keep_cols)))\n",
    "    return df_year\n",
    "\n",
    "\n",
    "df_set1 = prepare_set(train_set1, ds_size)\n",
    "df_test = prepare_set(test_set, ds_size, is_test_set=True)\n",
    "\n",
    "\n",
    "# Split test set in inliers and outliers\n",
    "df_test_inlier, df_test_outlier = split_set(\n",
    "    df_test, label_col_name=label_col_name, label_col_pos_val=label_col_pos_val\n",
    ")\n",
    "\n",
    "df_test = [(test_set, df_test_inlier, df_test_outlier),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load pretrained tokenizer and tokenize the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer_path = '../saved_tokenizers/kyoto-2016.json'\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"pad_token\": \"[PAD]\", \"unk_token\": \"[UNK]\", \"mask_token\": \"[MASK]\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prepare train and test datasets from the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function prepare_train_ds.<locals>.<lambda> at 0x7f8b709639a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping tokenizer on train\n",
      "Mapped tokenizer on train\n"
     ]
    }
   ],
   "source": [
    "from language_models.data_utils import prepare_train_ds, prepare_test_ds\n",
    "\n",
    "lm_ds_set1 = prepare_train_ds(\n",
    "    df_train=df_set1, tokenizer=tokenizer, block_size=len(cols)\n",
    ")\n",
    "\n",
    "ds_test = prepare_test_ds(\n",
    "    dfs_test=df_test, tokenizer=tokenizer, block_size=len(cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure IID model to train on set1 and finetune on set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 466774\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iid model on set1\n",
      "Training started...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9120' max='9120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9120/9120 07:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.878600</td>\n",
       "      <td>3.747528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.881100</td>\n",
       "      <td>2.707754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.210900</td>\n",
       "      <td>2.418274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.995300</td>\n",
       "      <td>2.284305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.873100</td>\n",
       "      <td>2.202164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_1.0/config.json\n",
      "Model weights saved in /tmp/_1.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 1.0: 3.747527599334717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_2.0/config.json\n",
      "Model weights saved in /tmp/_2.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 2.0: 2.7077536582946777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_3.0/config.json\n",
      "Model weights saved in /tmp/_3.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 3.0: 2.418274402618408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_4.0/config.json\n",
      "Model weights saved in /tmp/_4.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 4.0: 2.2843053340911865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_5.0/config.json\n",
      "Model weights saved in /tmp/_5.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 5.0: 2.2021636962890625\n",
      "{'inlier': Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 300000\n",
      "}), 'outlier': Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 74710\n",
      "})}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1170/1170 [02:26<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: inlier Anomaly score: 0.5981702628699448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 290/290 [00:35<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: outlier Anomaly score: 0.581571835178923\n",
      "ROC AUC       2008: 0.4345462931063264\n",
      "AUCPR INLIER  2008: 0.7633391538420858\n",
      "AUCPR OUTLIER 2008: 0.18097028621284614\n",
      "F1 INLIER 2008: 0.8897398041178619\n",
      "F1 OUTLIER 2008: 0.33142857142857146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdragoi/Documents/NeurIPS/Rebuttal/test_final/AnoShift/notebooks/../language_models/evaluation_utils.py:124: RuntimeWarning: invalid value encountered in divide\n",
      "  2 * (precision_inlier * recall_inlier) /\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from language_models.model_utils import configure_model, train_model\n",
    "from copy import deepcopy\n",
    "\n",
    "architecture = 'bert'\n",
    "pretrained = False\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "bs_train = 256\n",
    "bs_eval = 256\n",
    "num_epochs = 5\n",
    "\n",
    "model_iid = configure_model(\n",
    "        architecture=architecture,\n",
    "        pretrained=pretrained,\n",
    "        small=True,\n",
    "        vocab_size=vocab_size,\n",
    "        tokenizer=tokenizer,\n",
    "        embed_size=len(cols)\n",
    "    )\n",
    "\n",
    "print(\"Training iid model on set1\")\n",
    "train_model(\n",
    "    model=model_iid,\n",
    "    tokenizer=tokenizer,\n",
    "    ds_name='kyoto-2016',\n",
    "    train_set_name=f'{train_set1}',\n",
    "    run_name='iid',\n",
    "    lm_ds_train=lm_ds_set1,\n",
    "    lm_ds_eval=ds_test[0][1]['inlier'],\n",
    "    dss_test=ds_test,\n",
    "    save_model_path='/tmp/',\n",
    "    batch_size_train=bs_train,\n",
    "    batch_size_eval=bs_eval,\n",
    "    num_epochs=num_epochs,\n",
    "    tb_writer=None\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
