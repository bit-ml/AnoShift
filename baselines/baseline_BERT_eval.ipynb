{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of a pretrained BERT model on a data subset of Kyoto-2016\n",
    "<b> We show how to load a data subset, load a pretrained model and evaluate ROC_AUC, AUCPR_IN, AUCPR_OUT, F1_Inlier and F1_Outlier </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load Bert for Masked Language Modelling from checkpoint (we will load a model trained in iid mode on data containing sets 2006 to 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from  ../saved_models/bert_small_word_level/principal/kyoto-2016_subset_principal_iid_trainon_2006-2011_final/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "train_from = 2006\n",
    "train_until = 2011\n",
    "train_mode = 'iid'\n",
    "\n",
    "model_path = f'../saved_models/bert_small_word_level/principal/kyoto-2016_subset_principal_{train_mode}_trainon_{train_from}-{train_until}_final/'\n",
    "print(\"Loading model from \", model_path)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load dataframe for a given year that we will test on (we will use a subset of 300k samples from the 2013 set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test set: ../datasets/Kyoto-2016_AnoShift/subset/2013_subset.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "test_year = 2013\n",
    "ds_size = \"subset\"\n",
    "label_col_name = '18'\n",
    "label_col_pos_val = '1'\n",
    "\n",
    "# We only keep features 0 to 13\n",
    "cols = [str(i) for i in range(14)] + [label_col_name, ]\n",
    "\n",
    "test_year_path = f\"../datasets/Kyoto-2016_AnoShift/{ds_size}/{test_year}_{ds_size}.parquet\"\n",
    "print(\"Loading test set:\", test_year_path)\n",
    "\n",
    "df_test_year = pd.read_parquet(test_year_path, columns=cols)\n",
    "\n",
    "df_test = [(str(test_year), df_test_year),]\n",
    "\n",
    "\n",
    "from data_processor.data_loader import split_set\n",
    "\n",
    "# Split set in inliers and outliers\n",
    "df_test_inlier, df_test_outlier = split_set(\n",
    "        df_test_year, label_col_name=label_col_name, label_col_pos_val=label_col_pos_val\n",
    ")\n",
    "\n",
    "df_test = [(str(test_year), df_test_inlier, df_test_outlier),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We instantiate a Word Level Tokenizer preloaded from the checkpoint\n",
    "* The tokenizer contains the vocabulary for the kyoto-2016 dataset (which is of finite size, due to our binning schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer_path = '../saved_tokenizers/kyoto-2016.json'\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"pad_token\": \"[PAD]\", \"unk_token\": \"[UNK]\", \"mask_token\": \"[MASK]\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset object from the dataframes by tokenizing each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_models.data_utils import prepare_test_ds\n",
    "\n",
    "ds_test = prepare_test_ds(\n",
    "    dfs_test=df_test, tokenizer=tokenizer, block_size=len(cols)-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inlier': Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 299995\n",
      "}), 'outlier': Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1237765\n",
      "})}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1170/1170 [02:12<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: inlier Anomaly score: 0.4263514987247481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4834/4834 [09:03<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: outlier Anomaly score: 0.5505516537948927\n",
      "ROC AUC       2013: 0.8498379975910056\n",
      "AUCPR INLIER  2013: 0.5689764262412097\n",
      "AUCPR OUTLIER 2013: 0.9574600471765427\n",
      "F1 INLIER 2013: 0.5753914810648456\n",
      "F1 OUTLIER 2013: 0.9039597922799439\n"
     ]
    }
   ],
   "source": [
    "from language_models.evaluation_utils import eval_rocauc\n",
    "eval_rocauc(\n",
    "    model=model,\n",
    "    dss_test=ds_test,\n",
    "    bs_eval=256,\n",
    "    tokenizer=tokenizer,\n",
    "    epoch=0,\n",
    "    tb_writer=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a449927a1e23d59648888034a3ac7b7bcc61de3c536e2145e0eb73b4a17ef325"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
