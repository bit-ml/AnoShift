{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import collections\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support, precision_recall_curve\n",
    "\n",
    "pd.options.mode.chained_assignment = None \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "from base.base_dataset import BaseADDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "from deepSVDD import DeepSVDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_year(year):\n",
    "    if year <= 2010:\n",
    "        df = pd.read_parquet(f'../../datasets/Kyoto-2016_AnoShift/subset/{year}_subset.parquet',  engine='fastparquet')\n",
    "    else:\n",
    "        import sys\n",
    "        sys.exit(-1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def load_test_year(year):\n",
    "    if year <= 2010:\n",
    "        df = pd.read_parquet(f'../../datasets/Kyoto-2016_AnoShift/subset/{year}_subset_valid.parquet',  engine='fastparquet')\n",
    "    else:\n",
    "        df = pd.read_parquet(f'../../datasets/Kyoto-2016_AnoShift/subset/{year}_subset.parquet',  engine='fastparquet')\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def rename_columns(df):    \n",
    "    categorical_cols = [\"0\", \"1\", \"2\", \"3\", \"13\"]\n",
    "    numerical_cols = [\"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\"]\n",
    "    additional_cols = [\"14\", \"15\", \"16\", \"17\", \"19\"]\n",
    "    label_col = [\"18\"]\n",
    "\n",
    "    new_names = []\n",
    "    for col_name in df.columns.astype(str).values:\n",
    "        if col_name in numerical_cols:\n",
    "            df[col_name] = pd.to_numeric(df[col_name])\n",
    "            new_names.append((col_name, \"num_\" + col_name))\n",
    "        elif col_name in categorical_cols:\n",
    "            new_names.append((col_name, \"cat_\" + col_name))\n",
    "        elif col_name in additional_cols:\n",
    "            new_names.append((col_name, \"bonus_\" + col_name))\n",
    "        elif col_name in label_col:\n",
    "            df[col_name] = pd.to_numeric(df[col_name])\n",
    "            new_names.append((col_name, \"label\"))\n",
    "        else:\n",
    "            new_names.append((col_name, col_name))\n",
    "    df.rename(columns=dict(new_names), inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess(df, enc=None):\n",
    "    if not enc:\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        enc.fit(df.loc[:,['cat_' in i for i in df.columns]])\n",
    "    \n",
    "    num_cat_features = enc.transform(df.loc[:,['cat_' in i for i in df.columns]]).toarray()\n",
    "\n",
    "    df_catnum = pd.DataFrame(num_cat_features)\n",
    "    df_catnum = df_catnum.add_prefix('catnum_')\n",
    "\n",
    "    df.reset_index(drop=True)\n",
    "    df_new = pd.concat([df,  df_catnum], axis=1)\n",
    "    \n",
    "    \n",
    "    filter_clear = df_new[\"label\"] == 1\n",
    "    filter_infected = df_new[\"label\"] < 0\n",
    "    df_new[\"label\"][filter_clear] = 0\n",
    "    df_new[\"label\"][filter_infected] = 1\n",
    "\n",
    "    return df_new\n",
    "\n",
    "\n",
    "def print_results(labels, preds, text=\"?\", normalize=\"true\", th=0.5):\n",
    "    precision_anom, recall_anom, th_anom = precision_recall_curve(labels, preds, pos_label=1)\n",
    "    precision_norm, recall_norm, th_norm = precision_recall_curve(labels, 1-np.array(preds), pos_label=0)\n",
    "    \n",
    "    prec, recall, _, _ = precision_recall_fscore_support(labels, np.array(preds)>=th)\n",
    "    \n",
    "    # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "    pr_auc_norm = auc(recall_norm, precision_norm)\n",
    "    pr_auc_anom = auc(recall_anom, precision_anom)\n",
    "    \n",
    "    roc_auc = roc_auc_score(labels, preds)\n",
    "    \n",
    "    print(\"[%s] ROC-AUC     %.2f%% | PR-AUC-norm    %.2f%% | PR-AUC-anom    %.2f%%\" % (text, roc_auc*100, pr_auc_norm*100, pr_auc_anom*100))\n",
    "    return roc_auc*100, pr_auc_norm*100, pr_auc_anom*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df_year, ohe_enc):\n",
    "        df_year = preprocess(df_year, ohe_enc)\n",
    "        numerical_cols = df_year.columns.to_numpy()[['num_' in i for i in df_year.columns]]\n",
    "\n",
    "        x = df_year[numerical_cols].values\n",
    "        y = df_year[\"label\"].values\n",
    "\n",
    "        self.x_train = torch.tensor(x, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_train.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.targets[idx], idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate distance\n",
    "train_years = list(range(2006, 2011, 1))\n",
    "test_years = list(range(2006, 2016, 1))\n",
    "\n",
    "\n",
    "df_test_years = []\n",
    "df_train_years = []\n",
    "cats = []\n",
    "\n",
    "for year in train_years:\n",
    "    df_year = rename_columns(load_train_year(year))\n",
    "    print(\"train year\", year, df_year.shape)\n",
    "    cat = df_year.loc[:,['cat_' in i for i in df_year.columns]]\n",
    "    \n",
    "    df_train_years.append(df_year)\n",
    "    cats.append(cat)\n",
    "\n",
    "# enc only on train data\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(pd.concat(cats, axis=0))\n",
    "    \n",
    "for year in test_years:\n",
    "    df_year = rename_columns(load_test_year(year))\n",
    "    print(\"test year\", year, df_year.shape)\n",
    "    cat = df_year.loc[:,['cat_' in i for i in df_year.columns]]\n",
    "    \n",
    "    df_test_years.append(df_year)\n",
    "    cats.append(cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "train_datasets = []\n",
    "for_scaler = []\n",
    "\n",
    "for i_year, year in enumerate(train_years):\n",
    "    df_year = df_train_years[i_year]\n",
    "    ds = MyDataset(df_year, enc)\n",
    "    for_scaler.append(ds.x_train)\n",
    "    train_datasets.append(ds)\n",
    "\n",
    "scaler.fit(np.concatenate(for_scaler, axis=0))\n",
    "\n",
    "for i_year, year in enumerate(train_years):\n",
    "    train_datasets[i_year].x_train = torch.tensor(scaler.transform(train_datasets[i_year].x_train), dtype=torch.float32)\n",
    "    \n",
    "train_dataset = ConcatDataset(train_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KyotoDataset(BaseADDataset):\n",
    "    def __init__(self, root, train_set, test_set):\n",
    "        super().__init__(root)\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "\n",
    "    def loaders(self, batch_size: int, shuffle_train=True, shuffle_test=False, num_workers: int = 0) -> (\n",
    "            DataLoader, DataLoader):\n",
    "        train_loader = DataLoader(dataset=self.train_set, batch_size=batch_size, shuffle=shuffle_train,\n",
    "                                  num_workers=num_workers)\n",
    "        test_loader = DataLoader(dataset=self.test_set, batch_size=batch_size, shuffle=shuffle_test,\n",
    "                                 num_workers=num_workers)\n",
    "        return train_loader, test_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kyoto_train_ds = KyotoDataset(\"\", train_dataset, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "n_jobs_dataloader = 20\n",
    "net_name = \"kyoto\"\n",
    "\n",
    "nu = 0.2\n",
    "n_epochs = 2\n",
    "\n",
    "deep_SVDD = DeepSVDD(\"one-class\", nu)\n",
    "deep_SVDD.set_network(net_name)\n",
    "\n",
    "# Train model on dataset\n",
    "deep_SVDD.train(kyoto_train_ds,\n",
    "                optimizer_name=\"adam\",\n",
    "                n_epochs=n_epochs,\n",
    "                batch_size=10000,\n",
    "                device=device,\n",
    "                n_jobs_dataloader=n_jobs_dataloader)\n",
    "\n",
    "rocs, pr_norms, pr_anoms = [], [], []\n",
    "for i_year, year in enumerate(test_years):\n",
    "    df_year = df_test_years[i_year]\n",
    "    test_dataset = MyDataset(df_year, enc)\n",
    "    test_dataset.x_train = torch.tensor(scaler.transform(test_dataset.x_train), dtype=torch.float32)\n",
    "    kyoto_test_ds = KyotoDataset(\"\", test_dataset, test_dataset)\n",
    "\n",
    "    deep_SVDD.test(kyoto_test_ds, device=device, n_jobs_dataloader=n_jobs_dataloader)\n",
    "\n",
    "    # Plot most anomalous and most normal (within-class) test samples\n",
    "    indices, labels, scores = zip(*deep_SVDD.results['test_scores'])\n",
    "    indices, labels, scores = np.array(indices), np.array(labels), np.array(scores)\n",
    "\n",
    "    roc, pr_norm, pr_anom = print_results(labels, scores/scores.max(), text=\"deepSVDD \" + str(year), th=0.05, normalize=None)\n",
    "    rocs.append(roc)\n",
    "    pr_norms.append(pr_norm)\n",
    "    pr_anoms.append(pr_anom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "plt.plot(test_years, rocs, label='ROC-AUC')\n",
    "plt.plot(test_years, pr_norms, label='PR-AUC-inliers')\n",
    "plt.plot(test_years, pr_anoms, label='PR-AUC-outliers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a449927a1e23d59648888034a3ac7b7bcc61de3c536e2145e0eb73b4a17ef325"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
