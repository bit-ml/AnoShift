{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import collections\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support, precision_recall_curve\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_year(year):\n",
    "    if year <= 2010:\n",
    "        df = pd.read_parquet(f'../datasets/Kyoto-2016_AnoShift/subset/{year}_subset.parquet',  engine='fastparquet')\n",
    "    else:\n",
    "        import sys\n",
    "        sys.exit(-1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def load_test_year(year):\n",
    "    if year <= 2010:\n",
    "        df = pd.read_parquet(f'../datasets/Kyoto-2016_AnoShift/subset/{year}_subset_valid.parquet',  engine='fastparquet')\n",
    "    else:\n",
    "        df = pd.read_parquet(f'../datasets/Kyoto-2016_AnoShift/subset/{year}_subset.parquet',  engine='fastparquet')\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def rename_columns(df):    \n",
    "    categorical_cols = [\"0\", \"1\", \"2\", \"3\", \"13\"]\n",
    "    numerical_cols = [\"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\"]\n",
    "    additional_cols = [\"14\", \"15\", \"16\", \"17\", \"19\"]\n",
    "    label_col = [\"18\"]\n",
    "\n",
    "    new_names = []\n",
    "    for col_name in df.columns.astype(str).values:\n",
    "        if col_name in numerical_cols:\n",
    "            df[col_name] = pd.to_numeric(df[col_name])\n",
    "            new_names.append((col_name, \"num_\" + col_name))\n",
    "        elif col_name in categorical_cols:\n",
    "            new_names.append((col_name, \"cat_\" + col_name))\n",
    "        elif col_name in additional_cols:\n",
    "            new_names.append((col_name, \"bonus_\" + col_name))\n",
    "        elif col_name in label_col:\n",
    "            df[col_name] = pd.to_numeric(df[col_name])\n",
    "            new_names.append((col_name, \"label\"))\n",
    "        else:\n",
    "            new_names.append((col_name, col_name))\n",
    "    df.rename(columns=dict(new_names), inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess(df, enc=None):\n",
    "    if not enc:\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        enc.fit(df.loc[:,['cat_' in i for i in df.columns]])\n",
    "    \n",
    "    num_cat_features = enc.transform(df.loc[:,['cat_' in i for i in df.columns]]).toarray()\n",
    "\n",
    "    df_catnum = pd.DataFrame(num_cat_features)\n",
    "    df_catnum = df_catnum.add_prefix('catnum_')\n",
    "\n",
    "    df.reset_index(drop=True)\n",
    "    df_new = pd.concat([df,  df_catnum], axis=1)\n",
    "    \n",
    "    \n",
    "    filter_clear = df_new[\"label\"] == 1\n",
    "    filter_infected = df_new[\"label\"] < 0\n",
    "    df_new[\"label\"][filter_clear] = 0\n",
    "    df_new[\"label\"][filter_infected] = 1\n",
    "\n",
    "    return df_new, enc\n",
    "\n",
    "\n",
    "def print_results(labels, preds, text=\"?\", normalize=\"true\", th=0.5):\n",
    "    precision_anom, recall_anom, th_anom = precision_recall_curve(labels, preds, pos_label=1)\n",
    "    precision_norm, recall_norm, th_norm = precision_recall_curve(labels, 1-np.array(preds), pos_label=0)\n",
    "    \n",
    "    prec, recall, _, _ = precision_recall_fscore_support(labels, np.array(preds)>=th)\n",
    "    \n",
    "    # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "    pr_auc_norm = auc(recall_norm, precision_norm)\n",
    "    pr_auc_anom = auc(recall_anom, precision_anom)\n",
    "    \n",
    "    roc_auc = roc_auc_score(labels, preds)\n",
    "    \n",
    "    print(\"[%s] ROC-AUC     %.2f%% | PR-AUC-norm    %.2f%% | PR-AUC-anom    %.2f%%\" % (text, roc_auc*100, pr_auc_norm*100, pr_auc_anom*100))\n",
    "    return roc_auc*100, pr_auc_norm*100, pr_auc_anom*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(train_years):\n",
    "    dfs = []\n",
    "\n",
    "    for year in train_years:\n",
    "        df_year = load_train_year(year)\n",
    "        count_norm = df_year[df_year[\"18\"] == \"1\"].shape[0]\n",
    "        count_anomal = df_year[df_year[\"18\"] != \"1\"].shape[0]\n",
    "        print(year, \"normal:\", count_norm, \"anomalies:\", count_anomal)\n",
    "        dfs.append(df_year)\n",
    "    \n",
    "    print(\"Preprocess train data...\")\n",
    "    df_all_years = pd.concat(dfs, ignore_index=True)\n",
    "    df_all_years = rename_columns(df_all_years)\n",
    "    df_new, ohe_enc = preprocess(df_all_years)\n",
    "    \n",
    "    # split train + val\n",
    "    X_train, X_val = train_test_split(df_new, test_size=0.001, random_state=0, shuffle=True)\n",
    "\n",
    "    # select numerical features\n",
    "    numerical_cols = df_new.columns.to_numpy()[['num_' in i for i in df_new.columns]]\n",
    "\n",
    "    X_train_clear = X_train[X_train[\"label\"] == 0]\n",
    "    X_train_num = X_train_clear[numerical_cols].to_numpy()\n",
    "\n",
    "    X_val_num = X_val[numerical_cols].to_numpy()\n",
    "    y_val_num = X_val[\"label\"].to_numpy()\n",
    "    return X_train_num, X_val_num, y_val_num, numerical_cols, ohe_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pair_years = []\n",
    "pair_years.append(([2006, 2007, 2008, 2009, 2010], [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]))\n",
    "\n",
    "n_neighbors = 5\n",
    "scaler = RobustScaler()\n",
    "\n",
    "for train_years, test_years in pair_years:\n",
    "    rocs, pr_norms, pr_anoms = [], [], []\n",
    "    print(\"Train_years:\", train_years)\n",
    "    X_train_num, X_val_num, y_val_num, numerical_cols, ohe_enc = get_train(train_years)\n",
    "\n",
    "    print(\"Fit LocalOutlierFactor...\")\n",
    "    clf = LocalOutlierFactor(n_neighbors=n_neighbors,\n",
    "                             novelty=True,\n",
    "                             n_jobs=10)\n",
    "    scaler.fit(X_train_num)\n",
    "    clf.fit(scaler.transform(X_train_num))\n",
    "    del X_train_num\n",
    "\n",
    "    print(\"Done fitting.\")\n",
    "    \n",
    "    print(\"Test years:\", test_years)\n",
    "    for year in test_years:\n",
    "        df_year = load_test_year(year)\n",
    "        df_year = rename_columns(df_year)\n",
    "        df_test, _ = preprocess(df_year, ohe_enc)\n",
    "        X_test = df_test[numerical_cols].to_numpy()\n",
    "        y_test = df_test[\"label\"].to_numpy()\n",
    "\n",
    "        X_test = np.nan_to_num(X_test)\n",
    "        predict_test = (-1) * clf.score_samples(scaler.transform(X_test))\n",
    "        predict_test = np.nan_to_num(predict_test, 0)\n",
    "        y_test = np.nan_to_num(y_test, 0)\n",
    "        roc, pr_norm, pr_anom = print_results(y_test, predict_test, text=str(year), normalize=None, th=0.35)\n",
    "        rocs.append(roc)\n",
    "        pr_norms.append(pr_norm)\n",
    "        pr_anoms.append(pr_anom)\n",
    "        del df_test, df_year, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "plt.plot(test_years, rocs, label='ROC-AUC')\n",
    "plt.plot(test_years, pr_norms, label='PR-AUC-inliers')\n",
    "plt.plot(test_years, pr_anoms, label='PR-AUC-outliers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a449927a1e23d59648888034a3ac7b7bcc61de3c536e2145e0eb73b4a17ef325"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
