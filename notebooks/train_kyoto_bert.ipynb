{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train iid model on a subset and compare finetune and distil on a successive subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load data of 2006, 2007 and 2008 subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading set: ../datasets/Kyoto-2016_AnoShift/subset/2006_subset.parquet\n",
      "Loading set: ../datasets/Kyoto-2016_AnoShift/subset/2007_subset.parquet\n",
      "Loading set: ../datasets/Kyoto-2016_AnoShift/subset/2008_subset.parquet\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from data_processor.data_loader import split_set\n",
    "\n",
    "\n",
    "train_set1 = \"2006\"\n",
    "train_set2 = \"2007\"\n",
    "test_set = \"2008\"\n",
    "\n",
    "ds_size = \"subset\"\n",
    "label_col_name = '18'\n",
    "label_col_pos_val = '1'\n",
    "\n",
    "# We only keep features 0 to 13\n",
    "cols = [str(i) for i in range(14)]\n",
    "\n",
    "def prepare_set(year, ds_size, is_test_set=False):\n",
    "    keep_cols = cols\n",
    "\n",
    "    if is_test_set:\n",
    "        keep_cols += [label_col_name, ]\n",
    "\n",
    "    df_year_path = f\"../datasets/Kyoto-2016_AnoShift/{ds_size}/{year}_{ds_size}.parquet\"\n",
    "    print(\"Loading set:\", df_year_path)\n",
    "\n",
    "    df_year = pd.read_parquet(df_year_path)\n",
    "    df_year = df_year.drop(columns=list(set(df_year.columns) - set(keep_cols)))\n",
    "    return df_year\n",
    "\n",
    "\n",
    "df_set1 = prepare_set(train_set1, ds_size)\n",
    "df_set2 = prepare_set(train_set2, ds_size)\n",
    "df_test = prepare_set(test_set, ds_size, is_test_set=True)\n",
    "\n",
    "\n",
    "# Split test set in inliers and outliers\n",
    "df_test_inlier, df_test_outlier = split_set(\n",
    "    df_test, label_col_name=label_col_name, label_col_pos_val=label_col_pos_val\n",
    ")\n",
    "\n",
    "df_test = [(test_set, df_test_inlier, df_test_outlier),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load pretrained tokenizer and tokenize the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer_path = '../saved_tokenizers/kyoto-2016.json'\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_path)\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"pad_token\": \"[PAD]\", \"unk_token\": \"[UNK]\", \"mask_token\": \"[MASK]\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prepare train and test datasets from the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping tokenizer on train\n",
      "Mapped tokenizer on train\n",
      "Mapping tokenizer on train\n",
      "Mapped tokenizer on train\n"
     ]
    }
   ],
   "source": [
    "from language_models.data_utils import prepare_train_ds, prepare_test_ds\n",
    "\n",
    "lm_ds_set1 = prepare_train_ds(\n",
    "    df_train=df_set1, tokenizer=tokenizer, block_size=len(cols)\n",
    ")\n",
    "\n",
    "lm_ds_set2 = prepare_train_ds(\n",
    "    df_train=df_set2, tokenizer=tokenizer, block_size=len(cols)\n",
    ")\n",
    "\n",
    "ds_test = prepare_test_ds(\n",
    "    dfs_test=df_test, tokenizer=tokenizer, block_size=len(cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure IID model to train on set1 and finetune on set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 466774\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iid model on set1\n",
      "Training started...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9120' max='9120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9120/9120 06:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.842900</td>\n",
       "      <td>3.726619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.871000</td>\n",
       "      <td>2.695308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.210900</td>\n",
       "      <td>2.399026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.997300</td>\n",
       "      <td>2.258456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.879300</td>\n",
       "      <td>2.179849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_1.0/config.json\n",
      "Model weights saved in /tmp/_1.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 1.0: 3.726618766784668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_2.0/config.json\n",
      "Model weights saved in /tmp/_2.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 2.0: 2.695307731628418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_3.0/config.json\n",
      "Model weights saved in /tmp/_3.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 3.0: 2.3990261554718018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_4.0/config.json\n",
      "Model weights saved in /tmp/_4.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 4.0: 2.258456230163574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_5.0/config.json\n",
      "Model weights saved in /tmp/_5.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 5.0: 2.179849147796631\n",
      "{'inlier': Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 300000\n",
      "}), 'outlier': Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 74710\n",
      "})}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1170/1170 [02:27<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: inlier Anomaly score: 0.5905607052171541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:36<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: outlier Anomaly score: 0.580146057729217\n",
      "ROC AUC       2008: 0.4549984012224876\n",
      "AUCPR INLIER  2008: 0.7712426840717375\n",
      "AUCPR OUTLIER 2008: 0.1944036253780393\n",
      "F1 INLIER 2008: 0.8897796663051393\n",
      "F1 OUTLIER 2008: 0.33142857142857146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anomalybig/rebuttal/log-anomaly-detection/notebooks/../language_models/evaluation_utils.py:169: RuntimeWarning: invalid value encountered in true_divide\n",
      "  2 * (precision_inlier * recall_inlier) /\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from language_models.model_utils import configure_model, train_model\n",
    "from copy import deepcopy\n",
    "\n",
    "architecture = 'bert'\n",
    "pretrained = False\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "bs_train = 256\n",
    "bs_eval = 256\n",
    "num_epochs = 5\n",
    "\n",
    "model_iid = configure_model(\n",
    "        architecture=architecture,\n",
    "        pretrained=pretrained,\n",
    "        small=True,\n",
    "        vocab_size=vocab_size,\n",
    "        tokenizer=tokenizer,\n",
    "        embed_size=len(cols)\n",
    "    )\n",
    "\n",
    "print(\"Training iid model on set1\")\n",
    "train_model(\n",
    "    model=model_iid,\n",
    "    tokenizer=tokenizer,\n",
    "    ds_name='kyoto-2016',\n",
    "    train_set_name=f'{train_set1}',\n",
    "    run_name='iid',\n",
    "    lm_ds_train=lm_ds_set1,\n",
    "    lm_ds_eval=ds_test[0][1]['inlier'],\n",
    "    dss_test=ds_test,\n",
    "    save_model_path='/tmp/',\n",
    "    batch_size_train=bs_train,\n",
    "    batch_size_eval=bs_eval,\n",
    "    num_epochs=num_epochs,\n",
    "    tb_writer=None\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finetune iid model on set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning iid model on set2\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 415471\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8115\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8115' max='8115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8115/8115 05:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.860300</td>\n",
       "      <td>1.811404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.734500</td>\n",
       "      <td>1.749303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.670600</td>\n",
       "      <td>1.706480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.627000</td>\n",
       "      <td>1.678786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.588300</td>\n",
       "      <td>1.660579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_1.0/config.json\n",
      "Model weights saved in /tmp/_1.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 1.0: 1.8114039897918701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_2.0/config.json\n",
      "Model weights saved in /tmp/_2.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 2.0: 1.7493031024932861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_3.0/config.json\n",
      "Model weights saved in /tmp/_3.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 3.0: 1.706479787826538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_4.0/config.json\n",
      "Model weights saved in /tmp/_4.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 4.0: 1.678786039352417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300000\n",
      "  Batch size = 256\n",
      "Configuration saved in /tmp/_5.0/config.json\n",
      "Model weights saved in /tmp/_5.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss at epoch 5.0: 1.6605793237686157\n",
      "{'inlier': Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 300000\n",
      "}), 'outlier': Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 74710\n",
      "})}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1170/1170 [02:20<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: inlier Anomaly score: 0.5131177052078075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:34<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: outlier Anomaly score: 0.5579185805923171\n",
      "ROC AUC       2008: 0.6473447652288921\n",
      "AUCPR INLIER  2008: 0.8796352238806586\n",
      "AUCPR OUTLIER 2008: 0.28176339124610855\n",
      "F1 INLIER 2008: 0.8898239335091125\n",
      "F1 OUTLIER 2008: 0.3872355900357597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Finetuning iid model on set2\")\n",
    "model_finetune = deepcopy(model_iid)\n",
    "train_model(\n",
    "    model=model_iid,\n",
    "    tokenizer=tokenizer,\n",
    "    ds_name='kyoto-2016',\n",
    "    train_set_name=f'{train_set2}',\n",
    "    run_name='iid',\n",
    "    lm_ds_train=lm_ds_set2,\n",
    "    lm_ds_eval=ds_test[0][1]['inlier'],\n",
    "    dss_test=ds_test,\n",
    "    save_model_path='/tmp/',\n",
    "    batch_size_train=bs_train,\n",
    "    batch_size_eval=bs_eval,\n",
    "    num_epochs=num_epochs,\n",
    "    tb_writer=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distil iid model on set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:45<00:00, 35.86it/s]\n",
      "Configuration saved in /tmp/_1.0/config.json\n",
      "Model weights saved in /tmp/_1.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:48<00:00, 33.16it/s]\n",
      "Configuration saved in /tmp/_2.0/config.json\n",
      "Model weights saved in /tmp/_2.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:49<00:00, 32.74it/s]\n",
      "Configuration saved in /tmp/_3.0/config.json\n",
      "Model weights saved in /tmp/_3.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:49<00:00, 33.06it/s]\n",
      "Configuration saved in /tmp/_4.0/config.json\n",
      "Model weights saved in /tmp/_4.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1623/1623 [00:50<00:00, 31.98it/s]\n",
      "Configuration saved in /tmp/_5.0/config.json\n",
      "Model weights saved in /tmp/_5.0/pytorch_model.bin\n",
      "Configuration saved in /tmp/_final/config.json\n",
      "Model weights saved in /tmp/_final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inlier': Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 300000\n",
      "}), 'outlier': Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 74710\n",
      "})}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1170/1170 [02:23<00:00,  8.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: inlier Anomaly score: 0.5631501623717752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:36<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: outlier Anomaly score: 0.6124671218926347\n",
      "ROC AUC       2008: 0.6665836452278388\n",
      "AUCPR INLIER  2008: 0.8980555348518733\n",
      "AUCPR OUTLIER 2008: 0.2677317179252389\n",
      "F1 INLIER 2008: 0.8898073057719134\n",
      "F1 OUTLIER 2008: 0.4185166667910364\n"
     ]
    }
   ],
   "source": [
    "from language_models.model_utils import distil_model\n",
    "from language_models.data_utils import train_df_to_ds\n",
    "\n",
    "# Instantiate the teacher as the iid model checkpointed on set1\n",
    "teacher_model = deepcopy(model_iid)\n",
    "\n",
    "# Instantiate a new student model\n",
    "student_model = configure_model(\n",
    "        architecture=architecture,\n",
    "        pretrained=pretrained,\n",
    "        small=True,\n",
    "        vocab_size=vocab_size,\n",
    "        tokenizer=tokenizer,\n",
    "        embed_size=len(cols)\n",
    "    )\n",
    "\n",
    "ds_set2 = train_df_to_ds(df_set2)\n",
    "\n",
    "student_model = distil_model(\n",
    "        teacher=teacher_model,\n",
    "        student=student_model,\n",
    "        tokenizer=tokenizer,\n",
    "        ds_train=ds_set2,\n",
    "        dss_test=ds_test,\n",
    "        save_model_path='/tmp/',\n",
    "        batch_size_train=bs_train,\n",
    "        batch_size_eval=bs_eval,\n",
    "        num_epochs=num_epochs,\n",
    "        tb_writer=None\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a449927a1e23d59648888034a3ac7b7bcc61de3c536e2145e0eb73b4a17ef325"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
